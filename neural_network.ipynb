{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "211/211 [==============================] - 3s 7ms/step - loss: 9494.7832 - val_loss: 2688.7422\n",
      "Epoch 2/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2549.1687 - val_loss: 2656.9692\n",
      "Epoch 3/1000\n",
      "211/211 [==============================] - 2s 9ms/step - loss: 2416.0188 - val_loss: 2561.8525\n",
      "Epoch 4/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2363.3018 - val_loss: 2555.0652\n",
      "Epoch 5/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2334.1072 - val_loss: 2522.6997\n",
      "Epoch 6/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2313.1965 - val_loss: 2510.1963\n",
      "Epoch 7/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2296.4270 - val_loss: 2479.1816\n",
      "Epoch 8/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2284.6484 - val_loss: 2494.1389\n",
      "Epoch 9/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2287.3379 - val_loss: 2456.5986\n",
      "Epoch 10/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2273.1348 - val_loss: 2476.6360\n",
      "Epoch 11/1000\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 2259.2319 - val_loss: 2457.6196\n",
      "Epoch 12/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2264.3303 - val_loss: 2473.5239\n",
      "Epoch 13/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2264.4202 - val_loss: 2461.6890\n",
      "Epoch 14/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2247.3186 - val_loss: 2467.3054\n",
      "Epoch 15/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2256.6731 - val_loss: 2443.7217\n",
      "Epoch 16/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2241.9480 - val_loss: 2437.5259\n",
      "Epoch 17/1000\n",
      "211/211 [==============================] - 2s 8ms/step - loss: 2248.8376 - val_loss: 2474.1870\n",
      "Epoch 18/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2238.0757 - val_loss: 2445.8879\n",
      "Epoch 19/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2233.6785 - val_loss: 2465.3184\n",
      "Epoch 20/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2247.8457 - val_loss: 2457.2798\n",
      "Epoch 21/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2237.6934 - val_loss: 2455.6450\n",
      "Epoch 22/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2242.1548 - val_loss: 2456.4126\n",
      "Epoch 23/1000\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 2221.4333 - val_loss: 2438.1316\n",
      "Epoch 24/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2228.9270 - val_loss: 2439.6948\n",
      "Epoch 25/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2229.3867 - val_loss: 2426.3279\n",
      "Epoch 26/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2220.6545 - val_loss: 2443.3230\n",
      "Epoch 27/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2227.4326 - val_loss: 2438.2681\n",
      "Epoch 28/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2219.0188 - val_loss: 2436.4343\n",
      "Epoch 29/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2217.9128 - val_loss: 2437.7825\n",
      "Epoch 30/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2212.3357 - val_loss: 2465.3542\n",
      "Epoch 31/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2209.4204 - val_loss: 2461.1375\n",
      "Epoch 32/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2214.9192 - val_loss: 2454.4412\n",
      "Epoch 33/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2212.0898 - val_loss: 2432.4487\n",
      "Epoch 34/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2209.1033 - val_loss: 2429.2881\n",
      "Epoch 35/1000\n",
      "211/211 [==============================] - 2s 8ms/step - loss: 2208.6372 - val_loss: 2440.4158\n",
      "Epoch 36/1000\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 2208.2852 - val_loss: 2463.7639\n",
      "Epoch 37/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2208.5090 - val_loss: 2463.7363\n",
      "Epoch 38/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2197.1294 - val_loss: 2455.9944\n",
      "Epoch 39/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2201.0813 - val_loss: 2458.2390\n",
      "Epoch 40/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2201.5273 - val_loss: 2460.1553\n",
      "Epoch 41/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2198.7144 - val_loss: 2463.8711\n",
      "Epoch 42/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2194.3330 - val_loss: 2445.3889\n",
      "Epoch 43/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2198.8630 - val_loss: 2473.1082\n",
      "Epoch 44/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2198.8889 - val_loss: 2434.4407\n",
      "Epoch 45/1000\n",
      "211/211 [==============================] - 2s 8ms/step - loss: 2195.1584 - val_loss: 2463.5127\n",
      "Epoch 46/1000\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 2198.7812 - val_loss: 2446.5408\n",
      "Epoch 47/1000\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 2195.1348 - val_loss: 2452.2134\n",
      "Epoch 48/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2188.3987 - val_loss: 2453.2554\n",
      "Epoch 49/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2188.6514 - val_loss: 2453.8047\n",
      "Epoch 50/1000\n",
      "211/211 [==============================] - 2s 12ms/step - loss: 2188.1824 - val_loss: 2463.1450\n",
      "Epoch 51/1000\n",
      "211/211 [==============================] - 2s 9ms/step - loss: 2186.7964 - val_loss: 2467.8977\n",
      "Epoch 52/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2184.0435 - val_loss: 2413.3315\n",
      "Epoch 53/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2190.2058 - val_loss: 2455.4297\n",
      "Epoch 54/1000\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 2173.8005 - val_loss: 2416.7271\n",
      "Epoch 55/1000\n",
      "211/211 [==============================] - 2s 8ms/step - loss: 2173.3784 - val_loss: 2459.1301\n",
      "Epoch 56/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2174.5117 - val_loss: 2506.6118\n",
      "Epoch 57/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2178.9646 - val_loss: 2462.2617\n",
      "Epoch 58/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2173.3997 - val_loss: 2447.0940\n",
      "Epoch 59/1000\n",
      "211/211 [==============================] - 2s 8ms/step - loss: 2170.3784 - val_loss: 2452.7898\n",
      "Epoch 60/1000\n",
      "211/211 [==============================] - 2s 8ms/step - loss: 2174.0503 - val_loss: 2439.5679\n",
      "Epoch 61/1000\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 2172.5139 - val_loss: 2449.8330\n",
      "Epoch 62/1000\n",
      "211/211 [==============================] - 2s 9ms/step - loss: 2168.1003 - val_loss: 2456.4163\n",
      "Epoch 63/1000\n",
      "211/211 [==============================] - 2s 8ms/step - loss: 2169.6631 - val_loss: 2435.2085\n",
      "Epoch 64/1000\n",
      "211/211 [==============================] - 2s 8ms/step - loss: 2161.0923 - val_loss: 2466.1260\n",
      "Epoch 65/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2169.7178 - val_loss: 2465.8074\n",
      "Epoch 66/1000\n",
      "211/211 [==============================] - 2s 8ms/step - loss: 2166.1489 - val_loss: 2441.6367\n",
      "Epoch 67/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2166.2166 - val_loss: 2456.0527\n",
      "Epoch 68/1000\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 2158.1252 - val_loss: 2455.2751\n",
      "Epoch 69/1000\n",
      "211/211 [==============================] - 2s 8ms/step - loss: 2172.1953 - val_loss: 2426.1721\n",
      "Epoch 70/1000\n",
      "211/211 [==============================] - 2s 8ms/step - loss: 2162.5569 - val_loss: 2454.9028\n",
      "Epoch 71/1000\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 2162.0361 - val_loss: 2452.5420\n",
      "Epoch 72/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2156.3542 - val_loss: 2459.2883\n",
      "Epoch 73/1000\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 2158.4053 - val_loss: 2457.9075\n",
      "Epoch 74/1000\n",
      "211/211 [==============================] - 2s 8ms/step - loss: 2159.9412 - val_loss: 2451.0676\n",
      "Epoch 75/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2168.5427 - val_loss: 2466.0464\n",
      "Epoch 76/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2163.3738 - val_loss: 2438.2688\n",
      "Epoch 77/1000\n",
      "211/211 [==============================] - 2s 8ms/step - loss: 2160.6870 - val_loss: 2485.6521\n",
      "Epoch 78/1000\n",
      "211/211 [==============================] - 2s 8ms/step - loss: 2153.9492 - val_loss: 2456.1348\n",
      "Epoch 79/1000\n",
      "211/211 [==============================] - 2s 8ms/step - loss: 2150.5015 - val_loss: 2459.6138\n",
      "Epoch 80/1000\n",
      "211/211 [==============================] - 2s 8ms/step - loss: 2149.1526 - val_loss: 2471.8948\n",
      "Epoch 81/1000\n",
      "211/211 [==============================] - 2s 9ms/step - loss: 2153.0947 - val_loss: 2479.0671\n",
      "Epoch 82/1000\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 2154.3345 - val_loss: 2499.5361\n",
      "Epoch 83/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2146.7710 - val_loss: 2450.8208\n",
      "Epoch 84/1000\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 2151.0784 - val_loss: 2472.0540\n",
      "Epoch 85/1000\n",
      "211/211 [==============================] - 2s 9ms/step - loss: 2149.2859 - val_loss: 2487.3662\n",
      "Epoch 86/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2153.0110 - val_loss: 2467.2244\n",
      "Epoch 87/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2148.6965 - val_loss: 2464.0356\n",
      "Epoch 88/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2144.2817 - val_loss: 2458.9011\n",
      "Epoch 89/1000\n",
      "211/211 [==============================] - 2s 9ms/step - loss: 2144.1196 - val_loss: 2460.8220\n",
      "Epoch 90/1000\n",
      "211/211 [==============================] - 2s 8ms/step - loss: 2139.2935 - val_loss: 2468.6042\n",
      "Epoch 91/1000\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 2140.0337 - val_loss: 2480.2375\n",
      "Epoch 92/1000\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 2146.6748 - val_loss: 2470.4050\n",
      "Epoch 93/1000\n",
      "211/211 [==============================] - 2s 9ms/step - loss: 2151.1509 - val_loss: 2457.7490\n",
      "Epoch 94/1000\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 2143.9529 - val_loss: 2455.7759\n",
      "Epoch 95/1000\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 2141.2568 - val_loss: 2472.2188\n",
      "Epoch 96/1000\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 2141.5222 - val_loss: 2476.1663\n",
      "Epoch 97/1000\n",
      "211/211 [==============================] - 2s 8ms/step - loss: 2145.8364 - val_loss: 2495.3877\n",
      "Epoch 98/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2137.5830 - val_loss: 2471.3398\n",
      "Epoch 99/1000\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 2142.8591 - val_loss: 2475.6582\n",
      "Epoch 100/1000\n",
      "211/211 [==============================] - 2s 8ms/step - loss: 2149.5159 - val_loss: 2498.5681\n",
      "Epoch 101/1000\n",
      "211/211 [==============================] - 2s 8ms/step - loss: 2138.8206 - val_loss: 2468.8250\n",
      "Epoch 102/1000\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 2137.1370 - val_loss: 2464.9512\n",
      "Epoch 103/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2143.0291 - val_loss: 2473.7966\n",
      "Epoch 104/1000\n",
      "211/211 [==============================] - 2s 8ms/step - loss: 2140.2571 - val_loss: 2454.1743\n",
      "Epoch 105/1000\n",
      "211/211 [==============================] - 2s 11ms/step - loss: 2135.4514 - val_loss: 2490.3154\n",
      "Epoch 106/1000\n",
      "211/211 [==============================] - 2s 8ms/step - loss: 2143.2998 - val_loss: 2449.9233\n",
      "Epoch 107/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2137.5864 - val_loss: 2506.1489\n",
      "Epoch 108/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2137.8027 - val_loss: 2459.3123\n",
      "Epoch 109/1000\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 2136.4368 - val_loss: 2489.9348\n",
      "Epoch 110/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2139.8584 - val_loss: 2479.5715\n",
      "Epoch 111/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2135.0769 - val_loss: 2464.4297\n",
      "Epoch 112/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2130.0513 - val_loss: 2462.0520\n",
      "Epoch 113/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2135.4648 - val_loss: 2463.3257\n",
      "Epoch 114/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2130.8230 - val_loss: 2461.9961\n",
      "Epoch 115/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2134.7739 - val_loss: 2474.8894\n",
      "Epoch 116/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2122.9854 - val_loss: 2483.6106\n",
      "Epoch 117/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2127.8376 - val_loss: 2505.7139\n",
      "Epoch 118/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2140.2761 - val_loss: 2457.6609\n",
      "Epoch 119/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2135.7498 - val_loss: 2475.0017\n",
      "Epoch 120/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2132.5056 - val_loss: 2484.8855\n",
      "Epoch 121/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2129.6680 - val_loss: 2489.2966\n",
      "Epoch 122/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2123.3645 - val_loss: 2473.9497\n",
      "Epoch 123/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2125.3435 - val_loss: 2480.1646\n",
      "Epoch 124/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2131.9636 - val_loss: 2473.4983\n",
      "Epoch 125/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2128.1707 - val_loss: 2466.4429\n",
      "Epoch 126/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2124.2263 - val_loss: 2488.7434\n",
      "Epoch 127/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2141.2163 - val_loss: 2473.1719\n",
      "Epoch 128/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2130.8491 - val_loss: 2472.5691\n",
      "Epoch 129/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2127.3696 - val_loss: 2460.3047\n",
      "Epoch 130/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2126.7102 - val_loss: 2476.2148\n",
      "Epoch 131/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2126.1338 - val_loss: 2488.0732\n",
      "Epoch 132/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2120.2043 - val_loss: 2490.6743\n",
      "Epoch 133/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2123.2866 - val_loss: 2475.8228\n",
      "Epoch 134/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2122.6602 - val_loss: 2482.2014\n",
      "Epoch 135/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2128.1494 - val_loss: 2463.6514\n",
      "Epoch 136/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2127.7078 - val_loss: 2463.5356\n",
      "Epoch 137/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2134.1853 - val_loss: 2495.0120\n",
      "Epoch 138/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2123.1260 - val_loss: 2486.4031\n",
      "Epoch 139/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2123.4185 - val_loss: 2473.4185\n",
      "Epoch 140/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2127.4160 - val_loss: 2478.9846\n",
      "Epoch 141/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2124.3386 - val_loss: 2472.9390\n",
      "Epoch 142/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2121.5879 - val_loss: 2476.8079\n",
      "Epoch 143/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2126.8616 - val_loss: 2465.5962\n",
      "Epoch 144/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2123.5522 - val_loss: 2486.4917\n",
      "Epoch 145/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2124.3340 - val_loss: 2450.8303\n",
      "Epoch 146/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2126.6887 - val_loss: 2468.8953\n",
      "Epoch 147/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2116.2822 - val_loss: 2476.6016\n",
      "Epoch 148/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2120.3000 - val_loss: 2469.9585\n",
      "Epoch 149/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2122.1145 - val_loss: 2469.4141\n",
      "Epoch 150/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2119.7922 - val_loss: 2449.1033\n",
      "Epoch 151/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2116.6111 - val_loss: 2479.7229\n",
      "Epoch 152/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2121.4580 - val_loss: 2444.6978\n",
      "Epoch 153/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2122.3313 - val_loss: 2495.8699\n",
      "Epoch 154/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2128.1252 - val_loss: 2492.3030\n",
      "Epoch 155/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2118.8564 - val_loss: 2462.1499\n",
      "Epoch 156/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2118.6748 - val_loss: 2446.8303\n",
      "Epoch 157/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2128.9214 - val_loss: 2491.5864\n",
      "Epoch 158/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2115.0132 - val_loss: 2467.5930\n",
      "Epoch 159/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2116.9819 - val_loss: 2464.6157\n",
      "Epoch 160/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2121.4202 - val_loss: 2486.8843\n",
      "Epoch 161/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2114.2339 - val_loss: 2474.7119\n",
      "Epoch 162/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2118.5618 - val_loss: 2450.0769\n",
      "Epoch 163/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2120.6953 - val_loss: 2483.7766\n",
      "Epoch 164/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2110.3848 - val_loss: 2465.5571\n",
      "Epoch 165/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2122.2151 - val_loss: 2470.9500\n",
      "Epoch 166/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2117.0527 - val_loss: 2460.8716\n",
      "Epoch 167/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2124.2527 - val_loss: 2478.9988\n",
      "Epoch 168/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2123.8894 - val_loss: 2458.6074\n",
      "Epoch 169/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2117.8711 - val_loss: 2480.3225\n",
      "Epoch 170/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2121.0168 - val_loss: 2463.9773\n",
      "Epoch 171/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2120.0525 - val_loss: 2487.1021\n",
      "Epoch 172/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2116.7349 - val_loss: 2469.8103\n",
      "Epoch 173/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2121.8823 - val_loss: 2496.6519\n",
      "Epoch 174/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2131.9236 - val_loss: 2466.5249\n",
      "Epoch 175/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2114.5422 - val_loss: 2472.0493\n",
      "Epoch 176/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2117.4121 - val_loss: 2496.7625\n",
      "Epoch 177/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2112.0593 - val_loss: 2454.4058\n",
      "Epoch 178/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2112.0713 - val_loss: 2496.0898\n",
      "Epoch 179/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2115.6377 - val_loss: 2470.1780\n",
      "Epoch 180/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2110.3745 - val_loss: 2456.9824\n",
      "Epoch 181/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2111.7642 - val_loss: 2473.1521\n",
      "Epoch 182/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2111.9021 - val_loss: 2474.0374\n",
      "Epoch 183/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2111.6780 - val_loss: 2463.4346\n",
      "Epoch 184/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2111.1775 - val_loss: 2473.2998\n",
      "Epoch 185/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2116.2932 - val_loss: 2482.5369\n",
      "Epoch 186/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2115.0640 - val_loss: 2459.5276\n",
      "Epoch 187/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2114.0918 - val_loss: 2493.1194\n",
      "Epoch 188/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2110.2737 - val_loss: 2496.2312\n",
      "Epoch 189/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2113.2251 - val_loss: 2459.6973\n",
      "Epoch 190/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2109.4792 - val_loss: 2438.7729\n",
      "Epoch 191/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2108.9253 - val_loss: 2502.1685\n",
      "Epoch 192/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2113.6050 - val_loss: 2481.5061\n",
      "Epoch 193/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2118.6958 - val_loss: 2456.1294\n",
      "Epoch 194/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2111.5728 - val_loss: 2451.2803\n",
      "Epoch 195/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2112.8499 - val_loss: 2451.6475\n",
      "Epoch 196/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2121.3591 - val_loss: 2472.8110\n",
      "Epoch 197/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2114.1772 - val_loss: 2466.3889\n",
      "Epoch 198/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2115.4473 - val_loss: 2465.0576\n",
      "Epoch 199/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2112.7649 - val_loss: 2462.2366\n",
      "Epoch 200/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2110.8508 - val_loss: 2548.8569\n",
      "Epoch 201/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2114.9124 - val_loss: 2458.6531\n",
      "Epoch 202/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2112.3179 - val_loss: 2475.1084\n",
      "Epoch 203/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2111.8149 - val_loss: 2473.7607\n",
      "Epoch 204/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2118.9922 - val_loss: 2488.6355\n",
      "Epoch 205/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2112.8835 - val_loss: 2455.1479\n",
      "Epoch 206/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2114.3462 - val_loss: 2475.8472\n",
      "Epoch 207/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2105.8149 - val_loss: 2472.1160\n",
      "Epoch 208/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2112.6975 - val_loss: 2475.8657\n",
      "Epoch 209/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2112.0061 - val_loss: 2456.1274\n",
      "Epoch 210/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2105.7993 - val_loss: 2448.3289\n",
      "Epoch 211/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2107.8428 - val_loss: 2487.6355\n",
      "Epoch 212/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2114.0662 - val_loss: 2467.9607\n",
      "Epoch 213/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2115.3579 - val_loss: 2492.2815\n",
      "Epoch 214/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2112.5334 - val_loss: 2461.8186\n",
      "Epoch 215/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2107.7026 - val_loss: 2479.9114\n",
      "Epoch 216/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2108.2881 - val_loss: 2447.1636\n",
      "Epoch 217/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2111.2905 - val_loss: 2457.5076\n",
      "Epoch 218/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2108.5522 - val_loss: 2463.6194\n",
      "Epoch 219/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2107.5266 - val_loss: 2478.4214\n",
      "Epoch 220/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2106.2053 - val_loss: 2484.1392\n",
      "Epoch 221/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2107.7454 - val_loss: 2471.1716\n",
      "Epoch 222/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2111.2197 - val_loss: 2469.0742\n",
      "Epoch 223/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2108.2625 - val_loss: 2495.3130\n",
      "Epoch 224/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2116.5281 - val_loss: 2483.7529\n",
      "Epoch 225/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2113.4360 - val_loss: 2547.8574\n",
      "Epoch 226/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2109.9990 - val_loss: 2478.1431\n",
      "Epoch 227/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2112.5259 - val_loss: 2478.2913\n",
      "Epoch 228/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2112.5417 - val_loss: 2498.8406\n",
      "Epoch 229/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2117.0547 - val_loss: 2463.9475\n",
      "Epoch 230/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2105.0066 - val_loss: 2455.0227\n",
      "Epoch 231/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2113.3953 - val_loss: 2488.0139\n",
      "Epoch 232/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2103.3433 - val_loss: 2466.3547\n",
      "Epoch 233/1000\n",
      "211/211 [==============================] - 1s 3ms/step - loss: 2100.7502 - val_loss: 2535.1326\n",
      "Epoch 234/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2113.1265 - val_loss: 2472.1604\n",
      "Epoch 235/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2104.6189 - val_loss: 2476.3909\n",
      "Epoch 236/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2107.3550 - val_loss: 2459.7004\n",
      "Epoch 237/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2111.3726 - val_loss: 2487.7148\n",
      "Epoch 238/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2108.0298 - val_loss: 2470.6094\n",
      "Epoch 239/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2109.3806 - val_loss: 2455.7888\n",
      "Epoch 240/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2105.1670 - val_loss: 2488.7078\n",
      "Epoch 241/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2104.2927 - val_loss: 2440.0100\n",
      "Epoch 242/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2107.7051 - val_loss: 2457.7629\n",
      "Epoch 243/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2112.3440 - val_loss: 2475.1377\n",
      "Epoch 244/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2104.5132 - val_loss: 2497.8721\n",
      "Epoch 245/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2105.1492 - val_loss: 2456.1985\n",
      "Epoch 246/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2105.9338 - val_loss: 2468.7727\n",
      "Epoch 247/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2104.7739 - val_loss: 2488.1938\n",
      "Epoch 248/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2106.4553 - val_loss: 2470.9832\n",
      "Epoch 249/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2104.3354 - val_loss: 2482.2883\n",
      "Epoch 250/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2102.9751 - val_loss: 2491.2791\n",
      "Epoch 251/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2105.3564 - val_loss: 2500.6775\n",
      "Epoch 252/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2105.8115 - val_loss: 2487.9033\n",
      "Epoch 253/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2102.5432 - val_loss: 2483.8279\n",
      "Epoch 254/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2104.1707 - val_loss: 2451.4885\n",
      "Epoch 255/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2104.5083 - val_loss: 2505.1870\n",
      "Epoch 256/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2113.8391 - val_loss: 2499.3445\n",
      "Epoch 257/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2101.8535 - val_loss: 2472.5852\n",
      "Epoch 258/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2100.8892 - val_loss: 2480.9485\n",
      "Epoch 259/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2112.8250 - val_loss: 2490.4880\n",
      "Epoch 260/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2111.3640 - val_loss: 2473.9380\n",
      "Epoch 261/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2100.2090 - val_loss: 2511.7651\n",
      "Epoch 262/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2105.9768 - val_loss: 2485.8633\n",
      "Epoch 263/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2110.1309 - val_loss: 2493.0503\n",
      "Epoch 264/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2104.2256 - val_loss: 2489.3562\n",
      "Epoch 265/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2110.0688 - val_loss: 2482.2529\n",
      "Epoch 266/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2104.3955 - val_loss: 2503.4985\n",
      "Epoch 267/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2102.8081 - val_loss: 2510.5947\n",
      "Epoch 268/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2110.5261 - val_loss: 2499.9717\n",
      "Epoch 269/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2096.2434 - val_loss: 2504.6838\n",
      "Epoch 270/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2099.2085 - val_loss: 2482.1221\n",
      "Epoch 271/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2106.3635 - val_loss: 2520.4180\n",
      "Epoch 272/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2100.3291 - val_loss: 2509.6448\n",
      "Epoch 273/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2103.2651 - val_loss: 2506.1260\n",
      "Epoch 274/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2103.1704 - val_loss: 2482.5959\n",
      "Epoch 275/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2104.1655 - val_loss: 2536.2805\n",
      "Epoch 276/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2102.1968 - val_loss: 2516.1570\n",
      "Epoch 277/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2102.8911 - val_loss: 2470.8093\n",
      "Epoch 278/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2101.9832 - val_loss: 2480.0833\n",
      "Epoch 279/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2096.6902 - val_loss: 2515.2239\n",
      "Epoch 280/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2101.2480 - val_loss: 2518.8176\n",
      "Epoch 281/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2100.6868 - val_loss: 2482.7578\n",
      "Epoch 282/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2104.5176 - val_loss: 2489.6611\n",
      "Epoch 283/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2103.7979 - val_loss: 2527.7903\n",
      "Epoch 284/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2100.4382 - val_loss: 2558.3792\n",
      "Epoch 285/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2102.7417 - val_loss: 2484.7134\n",
      "Epoch 286/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2102.8667 - val_loss: 2528.7539\n",
      "Epoch 287/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2102.6558 - val_loss: 2488.6362\n",
      "Epoch 288/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2099.7971 - val_loss: 2527.4546\n",
      "Epoch 289/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2108.0627 - val_loss: 2493.9016\n",
      "Epoch 290/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2107.3826 - val_loss: 2459.0088\n",
      "Epoch 291/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2102.7495 - val_loss: 2529.3608\n",
      "Epoch 292/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2101.9468 - val_loss: 2509.9258\n",
      "Epoch 293/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2098.1802 - val_loss: 2503.8564\n",
      "Epoch 294/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2109.5325 - val_loss: 2482.3506\n",
      "Epoch 295/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2100.9207 - val_loss: 2478.3821\n",
      "Epoch 296/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2098.4277 - val_loss: 2482.5159\n",
      "Epoch 297/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2096.7119 - val_loss: 2467.0569\n",
      "Epoch 298/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2105.6091 - val_loss: 2598.9307\n",
      "Epoch 299/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2101.3757 - val_loss: 2470.9741\n",
      "Epoch 300/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2101.2449 - val_loss: 2515.1924\n",
      "Epoch 301/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2095.8745 - val_loss: 2482.7629\n",
      "Epoch 302/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2104.2317 - val_loss: 2480.3342\n",
      "Epoch 303/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2101.8391 - val_loss: 2490.5571\n",
      "Epoch 304/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2101.7017 - val_loss: 2491.5435\n",
      "Epoch 305/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2102.9482 - val_loss: 2527.7612\n",
      "Epoch 306/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2104.9885 - val_loss: 2511.7214\n",
      "Epoch 307/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2095.6067 - val_loss: 2515.6331\n",
      "Epoch 308/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2105.6145 - val_loss: 2461.3210\n",
      "Epoch 309/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2100.8691 - val_loss: 2543.4736\n",
      "Epoch 310/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2100.7925 - val_loss: 2536.4148\n",
      "Epoch 311/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2101.8335 - val_loss: 2501.8909\n",
      "Epoch 312/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2098.3586 - val_loss: 2510.4739\n",
      "Epoch 313/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2104.9141 - val_loss: 2623.6716\n",
      "Epoch 314/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2098.0654 - val_loss: 2475.4026\n",
      "Epoch 315/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2098.4470 - val_loss: 2528.0947\n",
      "Epoch 316/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2101.5859 - val_loss: 2493.8132\n",
      "Epoch 317/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2097.9702 - val_loss: 2491.3467\n",
      "Epoch 318/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2095.5891 - val_loss: 2551.3374\n",
      "Epoch 319/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2096.6565 - val_loss: 2468.2607\n",
      "Epoch 320/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2097.5984 - val_loss: 2593.0786\n",
      "Epoch 321/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2096.7520 - val_loss: 2525.0759\n",
      "Epoch 322/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2098.5242 - val_loss: 2524.2864\n",
      "Epoch 323/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2097.5527 - val_loss: 2536.1238\n",
      "Epoch 324/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2100.8687 - val_loss: 2569.4851\n",
      "Epoch 325/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2105.1628 - val_loss: 2506.3083\n",
      "Epoch 326/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2099.0784 - val_loss: 2531.2793\n",
      "Epoch 327/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2094.1709 - val_loss: 2512.7693\n",
      "Epoch 328/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2097.0894 - val_loss: 2560.0017\n",
      "Epoch 329/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2101.7773 - val_loss: 2518.2339\n",
      "Epoch 330/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2096.1851 - val_loss: 2520.5066\n",
      "Epoch 331/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2097.3618 - val_loss: 2504.6667\n",
      "Epoch 332/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2100.8667 - val_loss: 2589.9834\n",
      "Epoch 333/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2096.0374 - val_loss: 2513.9390\n",
      "Epoch 334/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2096.3501 - val_loss: 2510.2361\n",
      "Epoch 335/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2096.0513 - val_loss: 2526.5276\n",
      "Epoch 336/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2098.7275 - val_loss: 2486.5923\n",
      "Epoch 337/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2102.1753 - val_loss: 2481.7734\n",
      "Epoch 338/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2099.0654 - val_loss: 2510.9463\n",
      "Epoch 339/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2091.7136 - val_loss: 2486.7559\n",
      "Epoch 340/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2101.7065 - val_loss: 2633.7156\n",
      "Epoch 341/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2094.7141 - val_loss: 2544.0842\n",
      "Epoch 342/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2096.5032 - val_loss: 2483.6626\n",
      "Epoch 343/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2094.8762 - val_loss: 2496.6948\n",
      "Epoch 344/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2094.0786 - val_loss: 2508.6890\n",
      "Epoch 345/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2098.6147 - val_loss: 2501.3728\n",
      "Epoch 346/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2093.4568 - val_loss: 2545.1567\n",
      "Epoch 347/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2095.1328 - val_loss: 2528.1748\n",
      "Epoch 348/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2095.7031 - val_loss: 2490.3254\n",
      "Epoch 349/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2102.5054 - val_loss: 2503.1167\n",
      "Epoch 350/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2101.0015 - val_loss: 2486.7744\n",
      "Epoch 351/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2100.7646 - val_loss: 2492.4338\n",
      "Epoch 352/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2097.2344 - val_loss: 2551.5881\n",
      "Epoch 353/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2098.9727 - val_loss: 2514.4041\n",
      "Epoch 354/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2102.2976 - val_loss: 2492.4053\n",
      "Epoch 355/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2098.8914 - val_loss: 2467.4561\n",
      "Epoch 356/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2094.3591 - val_loss: 2457.0984\n",
      "Epoch 357/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2092.9358 - val_loss: 2537.4553\n",
      "Epoch 358/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2087.8862 - val_loss: 2554.8230\n",
      "Epoch 359/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2093.4155 - val_loss: 2483.3599\n",
      "Epoch 360/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2096.2119 - val_loss: 2466.3518\n",
      "Epoch 361/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2097.9548 - val_loss: 2544.6016\n",
      "Epoch 362/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2094.8401 - val_loss: 2522.7744\n",
      "Epoch 363/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2096.8804 - val_loss: 2543.5212\n",
      "Epoch 364/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2102.1633 - val_loss: 2497.9224\n",
      "Epoch 365/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2098.4065 - val_loss: 2483.7244\n",
      "Epoch 366/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2095.6970 - val_loss: 2501.2742\n",
      "Epoch 367/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2096.2700 - val_loss: 2462.2124\n",
      "Epoch 368/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2097.4448 - val_loss: 2471.6606\n",
      "Epoch 369/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2095.9043 - val_loss: 2478.9248\n",
      "Epoch 370/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2095.9751 - val_loss: 2478.3945\n",
      "Epoch 371/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2097.9880 - val_loss: 2463.6521\n",
      "Epoch 372/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2098.9775 - val_loss: 2461.9919\n",
      "Epoch 373/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2093.3396 - val_loss: 2512.1040\n",
      "Epoch 374/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2094.4224 - val_loss: 2465.4849\n",
      "Epoch 375/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2094.0444 - val_loss: 2535.3979\n",
      "Epoch 376/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2089.2625 - val_loss: 2500.8689\n",
      "Epoch 377/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2096.4377 - val_loss: 2516.8037\n",
      "Epoch 378/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2090.7244 - val_loss: 2457.1416\n",
      "Epoch 379/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2090.7268 - val_loss: 2471.4399\n",
      "Epoch 380/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2103.3469 - val_loss: 2467.9929\n",
      "Epoch 381/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2090.2981 - val_loss: 2457.5017\n",
      "Epoch 382/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2094.0820 - val_loss: 2518.2483\n",
      "Epoch 383/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2092.6946 - val_loss: 2455.7898\n",
      "Epoch 384/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2098.6748 - val_loss: 2479.5291\n",
      "Epoch 385/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2092.1919 - val_loss: 2479.1721\n",
      "Epoch 386/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2092.7808 - val_loss: 2544.4192\n",
      "Epoch 387/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2093.0874 - val_loss: 2498.3315\n",
      "Epoch 388/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2093.6047 - val_loss: 2484.9189\n",
      "Epoch 389/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2093.1021 - val_loss: 2445.4221\n",
      "Epoch 390/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2092.1450 - val_loss: 2469.5859\n",
      "Epoch 391/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2091.1055 - val_loss: 2485.5706\n",
      "Epoch 392/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2091.9919 - val_loss: 2479.8489\n",
      "Epoch 393/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2093.9343 - val_loss: 2480.6191\n",
      "Epoch 394/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2092.3633 - val_loss: 2520.1951\n",
      "Epoch 395/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2091.0105 - val_loss: 2453.6750\n",
      "Epoch 396/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2095.5474 - val_loss: 2545.0588\n",
      "Epoch 397/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2098.3547 - val_loss: 2467.7310\n",
      "Epoch 398/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2094.9590 - val_loss: 2480.1316\n",
      "Epoch 399/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2091.8540 - val_loss: 2479.5146\n",
      "Epoch 400/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2093.8794 - val_loss: 2465.7751\n",
      "Epoch 401/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2093.5520 - val_loss: 2476.7407\n",
      "Epoch 402/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2091.5742 - val_loss: 2449.6172\n",
      "Epoch 403/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2092.3008 - val_loss: 2503.7581\n",
      "Epoch 404/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2095.8054 - val_loss: 2458.1870\n",
      "Epoch 405/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2094.8723 - val_loss: 2497.7798\n",
      "Epoch 406/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2096.6882 - val_loss: 2448.1167\n",
      "Epoch 407/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2102.4175 - val_loss: 2467.5771\n",
      "Epoch 408/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2097.3674 - val_loss: 2468.7686\n",
      "Epoch 409/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2095.6194 - val_loss: 2533.2698\n",
      "Epoch 410/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2093.8752 - val_loss: 2484.4121\n",
      "Epoch 411/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2088.8364 - val_loss: 2465.3352\n",
      "Epoch 412/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2096.2263 - val_loss: 2441.1782\n",
      "Epoch 413/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2088.7876 - val_loss: 2456.9492\n",
      "Epoch 414/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2095.6729 - val_loss: 2472.2205\n",
      "Epoch 415/1000\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 2096.0176 - val_loss: 2471.2688\n",
      "Epoch 416/1000\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 2091.7278 - val_loss: 2496.3442\n",
      "Epoch 417/1000\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 2089.3657 - val_loss: 2469.7065\n",
      "Epoch 418/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2093.1997 - val_loss: 2443.8484\n",
      "Epoch 419/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2093.8486 - val_loss: 2482.0588\n",
      "Epoch 420/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2094.6484 - val_loss: 2441.3513\n",
      "Epoch 421/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2089.8616 - val_loss: 2480.0308\n",
      "Epoch 422/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2088.4814 - val_loss: 2512.5352\n",
      "Epoch 423/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2092.1079 - val_loss: 2526.6733\n",
      "Epoch 424/1000\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 2092.4487 - val_loss: 2454.9224\n",
      "Epoch 425/1000\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 2091.3806 - val_loss: 2507.5107\n",
      "Epoch 426/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2092.6140 - val_loss: 2488.1414\n",
      "Epoch 427/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2091.1257 - val_loss: 2545.2190\n",
      "Epoch 428/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2092.8059 - val_loss: 2493.8411\n",
      "Epoch 429/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2091.5830 - val_loss: 2572.7324\n",
      "Epoch 430/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2096.5046 - val_loss: 2479.4824\n",
      "Epoch 431/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2087.9165 - val_loss: 2516.6726\n",
      "Epoch 432/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2090.3655 - val_loss: 2567.3389\n",
      "Epoch 433/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2090.8501 - val_loss: 2625.2317\n",
      "Epoch 434/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2095.6553 - val_loss: 2483.3408\n",
      "Epoch 435/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2094.8328 - val_loss: 2517.9492\n",
      "Epoch 436/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2095.1633 - val_loss: 2452.4111\n",
      "Epoch 437/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2089.5745 - val_loss: 2469.6672\n",
      "Epoch 438/1000\n",
      "211/211 [==============================] - 2s 9ms/step - loss: 2092.3999 - val_loss: 2504.2195\n",
      "Epoch 439/1000\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 2087.7107 - val_loss: 2472.6992\n",
      "Epoch 440/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2093.8735 - val_loss: 2457.7937\n",
      "Epoch 441/1000\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 2091.2317 - val_loss: 2484.0308\n",
      "Epoch 442/1000\n",
      "211/211 [==============================] - 1s 3ms/step - loss: 2090.3022 - val_loss: 2455.4661\n",
      "Epoch 443/1000\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 2091.6216 - val_loss: 2477.7517\n",
      "Epoch 444/1000\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 2093.6870 - val_loss: 2464.0093\n",
      "Epoch 445/1000\n",
      "211/211 [==============================] - 1s 3ms/step - loss: 2091.5977 - val_loss: 2504.4614\n",
      "Epoch 446/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2093.5044 - val_loss: 2470.9507\n",
      "Epoch 447/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2089.9075 - val_loss: 2479.2615\n",
      "Epoch 448/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2091.9199 - val_loss: 2484.8472\n",
      "Epoch 449/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2091.1670 - val_loss: 2523.8330\n",
      "Epoch 450/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2093.6072 - val_loss: 2531.7703\n",
      "Epoch 451/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2088.1116 - val_loss: 2599.6462\n",
      "Epoch 452/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2090.0469 - val_loss: 2468.4084\n",
      "Epoch 453/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2087.1965 - val_loss: 2495.6873\n",
      "Epoch 454/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2091.3027 - val_loss: 2484.4814\n",
      "Epoch 455/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2090.8708 - val_loss: 2501.6003\n",
      "Epoch 456/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2089.9827 - val_loss: 2563.6575\n",
      "Epoch 457/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2090.0471 - val_loss: 2488.0515\n",
      "Epoch 458/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2092.8953 - val_loss: 2458.0728\n",
      "Epoch 459/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2094.5164 - val_loss: 2488.3765\n",
      "Epoch 460/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2094.1865 - val_loss: 2480.9932\n",
      "Epoch 461/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2089.6541 - val_loss: 2506.2661\n",
      "Epoch 462/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2090.4104 - val_loss: 2498.6597\n",
      "Epoch 463/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2090.6179 - val_loss: 2532.7888\n",
      "Epoch 464/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2089.5623 - val_loss: 2488.6096\n",
      "Epoch 465/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2090.2678 - val_loss: 2514.7151\n",
      "Epoch 466/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2085.7856 - val_loss: 2478.7065\n",
      "Epoch 467/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2092.6802 - val_loss: 2489.9780\n",
      "Epoch 468/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2096.3901 - val_loss: 2485.3225\n",
      "Epoch 469/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2089.5190 - val_loss: 2475.4592\n",
      "Epoch 470/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2089.5256 - val_loss: 2474.6655\n",
      "Epoch 471/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2087.9700 - val_loss: 2472.0288\n",
      "Epoch 472/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2089.9434 - val_loss: 2464.2502\n",
      "Epoch 473/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2086.9768 - val_loss: 2480.1353\n",
      "Epoch 474/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2090.3098 - val_loss: 2574.5989\n",
      "Epoch 475/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2091.2471 - val_loss: 2465.2605\n",
      "Epoch 476/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2089.0950 - val_loss: 2480.6802\n",
      "Epoch 477/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2090.8948 - val_loss: 2459.2363\n",
      "Epoch 478/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2095.1162 - val_loss: 2772.1121\n",
      "Epoch 479/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2087.7712 - val_loss: 2583.4041\n",
      "Epoch 480/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2088.4739 - val_loss: 2662.5461\n",
      "Epoch 481/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2091.8970 - val_loss: 2473.2302\n",
      "Epoch 482/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2093.0098 - val_loss: 2467.4026\n",
      "Epoch 483/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2090.9463 - val_loss: 2554.4226\n",
      "Epoch 484/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2088.4014 - val_loss: 2478.1780\n",
      "Epoch 485/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2090.8870 - val_loss: 2550.2490\n",
      "Epoch 486/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2087.7053 - val_loss: 2499.7415\n",
      "Epoch 487/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2086.0413 - val_loss: 2454.3262\n",
      "Epoch 488/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2082.0710 - val_loss: 2466.6387\n",
      "Epoch 489/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2091.8142 - val_loss: 2516.1128\n",
      "Epoch 490/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2088.3171 - val_loss: 2705.8508\n",
      "Epoch 491/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2091.9319 - val_loss: 2484.5339\n",
      "Epoch 492/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2088.5754 - val_loss: 2532.0278\n",
      "Epoch 493/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2089.9915 - val_loss: 2474.3770\n",
      "Epoch 494/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2087.1057 - val_loss: 2628.5396\n",
      "Epoch 495/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2087.1140 - val_loss: 2522.0024\n",
      "Epoch 496/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2085.9314 - val_loss: 2715.6484\n",
      "Epoch 497/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2085.6169 - val_loss: 2454.4800\n",
      "Epoch 498/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2095.9729 - val_loss: 2651.5762\n",
      "Epoch 499/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2092.3320 - val_loss: 2451.5464\n",
      "Epoch 500/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2089.2329 - val_loss: 2694.1284\n",
      "Epoch 501/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2089.1362 - val_loss: 2543.2744\n",
      "Epoch 502/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2087.0352 - val_loss: 2487.8789\n",
      "Epoch 503/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2092.1477 - val_loss: 2456.5588\n",
      "Epoch 504/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2089.9844 - val_loss: 2556.2349\n",
      "Epoch 505/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2086.6201 - val_loss: 2481.3760\n",
      "Epoch 506/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2089.4841 - val_loss: 2537.2297\n",
      "Epoch 507/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2086.7039 - val_loss: 2663.2896\n",
      "Epoch 508/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2085.7974 - val_loss: 2543.7085\n",
      "Epoch 509/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2083.4382 - val_loss: 2520.8513\n",
      "Epoch 510/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2092.2407 - val_loss: 2605.6340\n",
      "Epoch 511/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2092.6089 - val_loss: 2468.6052\n",
      "Epoch 512/1000\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 2081.1536 - val_loss: 2698.8447\n",
      "Epoch 513/1000\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 2085.4041 - val_loss: 2470.2341\n",
      "Epoch 514/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2086.8149 - val_loss: 2768.7585\n",
      "Epoch 515/1000\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 2094.7307 - val_loss: 2518.0283\n",
      "Epoch 516/1000\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 2087.0002 - val_loss: 2468.2920\n",
      "Epoch 517/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2091.5518 - val_loss: 2478.2683\n",
      "Epoch 518/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2084.1089 - val_loss: 2507.1255\n",
      "Epoch 519/1000\n",
      "211/211 [==============================] - 1s 3ms/step - loss: 2088.0615 - val_loss: 2501.8674\n",
      "Epoch 520/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2083.2983 - val_loss: 2476.9409\n",
      "Epoch 521/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2085.6824 - val_loss: 2592.0439\n",
      "Epoch 522/1000\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 2086.7974 - val_loss: 2457.4055\n",
      "Epoch 523/1000\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 2087.2542 - val_loss: 2575.0029\n",
      "Epoch 524/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2089.2124 - val_loss: 2469.9705\n",
      "Epoch 525/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2087.7026 - val_loss: 2540.1348\n",
      "Epoch 526/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2084.2278 - val_loss: 2467.8325\n",
      "Epoch 527/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2086.8867 - val_loss: 2477.3911\n",
      "Epoch 528/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2086.2971 - val_loss: 2490.2029\n",
      "Epoch 529/1000\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 2086.2866 - val_loss: 2662.1736\n",
      "Epoch 530/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2088.4321 - val_loss: 2501.9253\n",
      "Epoch 531/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2087.6350 - val_loss: 2551.3364\n",
      "Epoch 532/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2084.3538 - val_loss: 2462.1987\n",
      "Epoch 533/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2084.7395 - val_loss: 2511.1611\n",
      "Epoch 534/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2090.6729 - val_loss: 2535.7266\n",
      "Epoch 535/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2084.8821 - val_loss: 2488.7471\n",
      "Epoch 536/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2087.5591 - val_loss: 2499.9277\n",
      "Epoch 537/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2095.0562 - val_loss: 2483.4673\n",
      "Epoch 538/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2090.3728 - val_loss: 2463.4512\n",
      "Epoch 539/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2087.0813 - val_loss: 2600.4343\n",
      "Epoch 540/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2087.3513 - val_loss: 2528.5483\n",
      "Epoch 541/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2087.3594 - val_loss: 2525.3267\n",
      "Epoch 542/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2086.7766 - val_loss: 2472.0798\n",
      "Epoch 543/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2087.6614 - val_loss: 2611.1067\n",
      "Epoch 544/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2092.6001 - val_loss: 2557.8606\n",
      "Epoch 545/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2089.2986 - val_loss: 2691.8870\n",
      "Epoch 546/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2083.9702 - val_loss: 2475.8147\n",
      "Epoch 547/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2089.1685 - val_loss: 2624.1289\n",
      "Epoch 548/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2084.0774 - val_loss: 2500.7607\n",
      "Epoch 549/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2087.7073 - val_loss: 2494.3647\n",
      "Epoch 550/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2088.9294 - val_loss: 2477.3203\n",
      "Epoch 551/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2082.7566 - val_loss: 2675.8735\n",
      "Epoch 552/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2087.3584 - val_loss: 2456.2041\n",
      "Epoch 553/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2095.2275 - val_loss: 2627.2224\n",
      "Epoch 554/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2085.1067 - val_loss: 2721.7429\n",
      "Epoch 555/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2089.2029 - val_loss: 2469.2180\n",
      "Epoch 556/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2083.2847 - val_loss: 2652.6895\n",
      "Epoch 557/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2085.1069 - val_loss: 2609.2434\n",
      "Epoch 558/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2090.5664 - val_loss: 2503.5891\n",
      "Epoch 559/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2084.3145 - val_loss: 2538.4302\n",
      "Epoch 560/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2086.7617 - val_loss: 2565.7578\n",
      "Epoch 561/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2084.1440 - val_loss: 2469.3120\n",
      "Epoch 562/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2085.3416 - val_loss: 2610.9663\n",
      "Epoch 563/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2092.6875 - val_loss: 2538.1084\n",
      "Epoch 564/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2087.5623 - val_loss: 2486.5969\n",
      "Epoch 565/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2085.4524 - val_loss: 2459.5676\n",
      "Epoch 566/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2087.8430 - val_loss: 2702.4207\n",
      "Epoch 567/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2085.4561 - val_loss: 2559.5693\n",
      "Epoch 568/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2087.7876 - val_loss: 2543.9839\n",
      "Epoch 569/1000\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 2086.0339 - val_loss: 2578.9097\n",
      "Epoch 570/1000\n",
      "211/211 [==============================] - 2s 8ms/step - loss: 2089.4097 - val_loss: 2458.3547\n",
      "Epoch 571/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2084.7751 - val_loss: 2471.0420\n",
      "Epoch 572/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2086.2290 - val_loss: 2588.6494\n",
      "Epoch 573/1000\n",
      "211/211 [==============================] - 2s 10ms/step - loss: 2083.3311 - val_loss: 2475.0989\n",
      "Epoch 574/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2084.9736 - val_loss: 2712.5525\n",
      "Epoch 575/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2084.3701 - val_loss: 2486.5842\n",
      "Epoch 576/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2084.7791 - val_loss: 2599.0444\n",
      "Epoch 577/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2086.5774 - val_loss: 2534.4624\n",
      "Epoch 578/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2086.8887 - val_loss: 2520.3762\n",
      "Epoch 579/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2086.2737 - val_loss: 2505.0098\n",
      "Epoch 580/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2085.5027 - val_loss: 2509.4329\n",
      "Epoch 581/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2082.8613 - val_loss: 2494.3503\n",
      "Epoch 582/1000\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 2085.9756 - val_loss: 2468.0879\n",
      "Epoch 583/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2084.1187 - val_loss: 2514.0034\n",
      "Epoch 584/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2091.8694 - val_loss: 2477.8340\n",
      "Epoch 585/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2086.8220 - val_loss: 2573.2820\n",
      "Epoch 586/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2084.5547 - val_loss: 2512.0964\n",
      "Epoch 587/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2085.4143 - val_loss: 2607.7200\n",
      "Epoch 588/1000\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 2083.7361 - val_loss: 2473.1853\n",
      "Epoch 589/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2085.1345 - val_loss: 2556.7476\n",
      "Epoch 590/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2083.1279 - val_loss: 2641.1921\n",
      "Epoch 591/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2082.0139 - val_loss: 2600.7175\n",
      "Epoch 592/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2082.5864 - val_loss: 2524.3230\n",
      "Epoch 593/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2090.7383 - val_loss: 2488.5261\n",
      "Epoch 594/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2088.4556 - val_loss: 2591.0413\n",
      "Epoch 595/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2082.8101 - val_loss: 2613.3162\n",
      "Epoch 596/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2088.2673 - val_loss: 2725.8181\n",
      "Epoch 597/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2087.5564 - val_loss: 2519.9851\n",
      "Epoch 598/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2084.9719 - val_loss: 2665.9338\n",
      "Epoch 599/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2084.3167 - val_loss: 2740.8071\n",
      "Epoch 600/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2079.9983 - val_loss: 2664.0134\n",
      "Epoch 601/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2088.7759 - val_loss: 2669.0020\n",
      "Epoch 602/1000\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 2086.2356 - val_loss: 2465.2378\n",
      "Epoch 603/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2084.1069 - val_loss: 2666.9236\n",
      "Epoch 604/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2085.3530 - val_loss: 2578.4839\n",
      "Epoch 605/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2084.4275 - val_loss: 2621.4050\n",
      "Epoch 606/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2082.1396 - val_loss: 2496.9067\n",
      "Epoch 607/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2088.0869 - val_loss: 2628.6348\n",
      "Epoch 608/1000\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 2089.1851 - val_loss: 2814.8889\n",
      "Epoch 609/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2083.0254 - val_loss: 2558.9924\n",
      "Epoch 610/1000\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 2083.2483 - val_loss: 2481.7117\n",
      "Epoch 611/1000\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 2087.2825 - val_loss: 2503.6365\n",
      "Epoch 612/1000\n",
      "211/211 [==============================] - 1s 3ms/step - loss: 2085.5232 - val_loss: 2484.8479\n",
      "Epoch 613/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2086.4287 - val_loss: 2456.9307\n",
      "Epoch 614/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2084.9246 - val_loss: 2577.6807\n",
      "Epoch 615/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2082.1086 - val_loss: 2538.7930\n",
      "Epoch 616/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2083.7419 - val_loss: 2568.5056\n",
      "Epoch 617/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2088.1943 - val_loss: 2487.2698\n",
      "Epoch 618/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2082.8733 - val_loss: 2576.1016\n",
      "Epoch 619/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2084.2708 - val_loss: 2645.9382\n",
      "Epoch 620/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2083.1006 - val_loss: 2610.7188\n",
      "Epoch 621/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2086.2166 - val_loss: 2469.4866\n",
      "Epoch 622/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2087.1550 - val_loss: 2568.3301\n",
      "Epoch 623/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2081.3262 - val_loss: 2532.0620\n",
      "Epoch 624/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2084.6240 - val_loss: 2882.8188\n",
      "Epoch 625/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2091.5566 - val_loss: 2683.0535\n",
      "Epoch 626/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2083.5249 - val_loss: 2467.9485\n",
      "Epoch 627/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2082.6147 - val_loss: 2536.1599\n",
      "Epoch 628/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2082.3909 - val_loss: 2726.7065\n",
      "Epoch 629/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2086.3347 - val_loss: 2741.9751\n",
      "Epoch 630/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2085.8027 - val_loss: 2624.2759\n",
      "Epoch 631/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2083.7246 - val_loss: 2481.3997\n",
      "Epoch 632/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2084.4868 - val_loss: 2595.5352\n",
      "Epoch 633/1000\n",
      "211/211 [==============================] - 2s 8ms/step - loss: 2077.5527 - val_loss: 2543.9636\n",
      "Epoch 634/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2085.7188 - val_loss: 2746.2776\n",
      "Epoch 635/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2085.0024 - val_loss: 2502.5171\n",
      "Epoch 636/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2086.0850 - val_loss: 2494.2168\n",
      "Epoch 637/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2088.2759 - val_loss: 2602.4790\n",
      "Epoch 638/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2082.4211 - val_loss: 2717.5779\n",
      "Epoch 639/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2087.3325 - val_loss: 2781.0901\n",
      "Epoch 640/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2084.8186 - val_loss: 2484.8171\n",
      "Epoch 641/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2084.3303 - val_loss: 2677.6221\n",
      "Epoch 642/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2082.6604 - val_loss: 2501.2502\n",
      "Epoch 643/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2082.3772 - val_loss: 2497.5642\n",
      "Epoch 644/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2084.1487 - val_loss: 2615.3352\n",
      "Epoch 645/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2086.2180 - val_loss: 2599.0508\n",
      "Epoch 646/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2086.3940 - val_loss: 2560.9844\n",
      "Epoch 647/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2085.2078 - val_loss: 2741.1743\n",
      "Epoch 648/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2083.7427 - val_loss: 2591.2275\n",
      "Epoch 649/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2086.9116 - val_loss: 2827.2593\n",
      "Epoch 650/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2084.7651 - val_loss: 2461.8003\n",
      "Epoch 651/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2081.3503 - val_loss: 2496.2363\n",
      "Epoch 652/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2080.0681 - val_loss: 2596.6509\n",
      "Epoch 653/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2081.8191 - val_loss: 2532.1387\n",
      "Epoch 654/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2083.1772 - val_loss: 2934.1445\n",
      "Epoch 655/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2081.1975 - val_loss: 2464.7876\n",
      "Epoch 656/1000\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 2084.7795 - val_loss: 2565.7820\n",
      "Epoch 657/1000\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 2081.3621 - val_loss: 2481.3115\n",
      "Epoch 658/1000\n",
      " 32/211 [===>..........................] - ETA: 0s - loss: 2119.5051"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test_scaled, y_test)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('jobs_in_data.csv')\n",
    "\n",
    "# Drop the 'salary_in_usd' column since we're ignoring it\n",
    "data.drop(columns=['salary_in_usd'], inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "data = pd.get_dummies(data)\n",
    "# Divide the 'salary' column by 1000\n",
    "data['salary'] /= 1000\n",
    "# Separate features and target\n",
    "X = data.drop(columns=['salary'])\n",
    "y = data['salary']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    # Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    # Dropout(0.2),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model with a custom learning rate\n",
    "# custom_learning_rate = 0.001  # Set your desired learning rate here\n",
    "# model.compile(optimizer=Adam(learning_rate=custom_learning_rate), loss='mean_squared_error')\n",
    "model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=1000, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print('Test Loss:', loss)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test_scaled).flatten()\n",
    "\n",
    "# Visualize Predictions vs. Actual Values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, predictions, alpha=0.5)\n",
    "plt.xlabel('Actual Salary')\n",
    "plt.ylabel('Predicted Salary')\n",
    "plt.title('Actual vs. Predicted Salaries')\n",
    "plt.show()\n",
    "\n",
    "# Calculate Evaluation Metrics\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "print('Mean Absolute Error:', mae)\n",
    "print('Mean Squared Error:', mse)\n",
    "print('Root Mean Squared Error:', rmse)\n",
    "\n",
    "# Feature Importance\n",
    "# Get the weights of the first layer\n",
    "weights = model.layers[0].get_weights()[0]\n",
    "\n",
    "# Calculate the importance of features based on the weights\n",
    "feature_importance = np.mean(np.abs(weights), axis=0)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = X.columns\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_indices = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "# Top 10 features\n",
    "top_features = feature_names[sorted_indices][:10]\n",
    "print('Top 10 Features:', top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2813702912.0 0.001\n",
    " 2603507200.0 0.01\n",
    " 3033694464.0 0.1\n",
    " 2603507017.888839 0.01\n",
    "\n",
    " 1000 epoch\n",
    " 2596519395.2590485\n",
    "root 50956.05356833522\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
