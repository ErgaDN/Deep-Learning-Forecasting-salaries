{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SpatialDropout1D, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4212d23db6073cac"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-27T09:44:52.884916400Z",
     "start_time": "2024-02-27T09:42:59.531680700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "211/211 [==============================] - 5s 6ms/step - loss: 24067.1445 - val_loss: 20625.2129\n",
      "Epoch 2/100\n",
      "211/211 [==============================] - 1s 3ms/step - loss: 18259.9883 - val_loss: 16715.4238\n",
      "Epoch 3/100\n",
      "211/211 [==============================] - 1s 3ms/step - loss: 15393.8867 - val_loss: 14337.2715\n",
      "Epoch 4/100\n",
      "211/211 [==============================] - 1s 3ms/step - loss: 13257.6543 - val_loss: 12413.7930\n",
      "Epoch 5/100\n",
      "211/211 [==============================] - 1s 3ms/step - loss: 11497.1729 - val_loss: 10808.6973\n",
      "Epoch 6/100\n",
      "211/211 [==============================] - 1s 3ms/step - loss: 10022.1426 - val_loss: 9457.0693\n",
      "Epoch 7/100\n",
      "211/211 [==============================] - 1s 3ms/step - loss: 8783.6318 - val_loss: 8327.4297\n",
      "Epoch 8/100\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 7748.6636 - val_loss: 7383.0596\n",
      "Epoch 9/100\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 6890.0396 - val_loss: 6604.7256\n",
      "Epoch 10/100\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 6186.6631 - val_loss: 5975.4458\n",
      "Epoch 11/100\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 5616.1035 - val_loss: 5466.1406\n",
      "Epoch 12/100\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 5162.7925 - val_loss: 5064.7529\n",
      "Epoch 13/100\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 4810.4111 - val_loss: 4757.2227\n",
      "Epoch 14/100\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 4541.7124 - val_loss: 4526.9790\n",
      "Epoch 15/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 4342.0972 - val_loss: 4358.1870\n",
      "Epoch 16/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 4199.4263 - val_loss: 4238.5200\n",
      "Epoch 17/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 4100.8877 - val_loss: 4158.4604\n",
      "Epoch 18/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 4035.1479 - val_loss: 4106.9922\n",
      "Epoch 19/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3993.8604 - val_loss: 4074.4502\n",
      "Epoch 20/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3968.9033 - val_loss: 4056.4043\n",
      "Epoch 21/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3954.7031 - val_loss: 4045.5581\n",
      "Epoch 22/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3947.0386 - val_loss: 4039.2290\n",
      "Epoch 23/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3898.2708 - val_loss: 3931.4988\n",
      "Epoch 24/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3853.4636 - val_loss: 3908.8071\n",
      "Epoch 25/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3844.8501 - val_loss: 3896.9121\n",
      "Epoch 26/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3833.7639 - val_loss: 3894.1479\n",
      "Epoch 27/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3835.2502 - val_loss: 3891.5544\n",
      "Epoch 28/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3828.6096 - val_loss: 3875.8691\n",
      "Epoch 29/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3829.2751 - val_loss: 3876.1125\n",
      "Epoch 30/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3825.4287 - val_loss: 3876.2898\n",
      "Epoch 31/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3832.9626 - val_loss: 3883.9475\n",
      "Epoch 32/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3827.7354 - val_loss: 3879.9556\n",
      "Epoch 33/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3826.6765 - val_loss: 3873.7651\n",
      "Epoch 34/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3825.6655 - val_loss: 3878.7512\n",
      "Epoch 35/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3827.9629 - val_loss: 3875.8403\n",
      "Epoch 36/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3828.7644 - val_loss: 3875.3110\n",
      "Epoch 37/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3827.9434 - val_loss: 3877.0564\n",
      "Epoch 38/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3820.7964 - val_loss: 3875.0764\n",
      "Epoch 39/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3824.1760 - val_loss: 3883.0461\n",
      "Epoch 40/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3828.7563 - val_loss: 3878.2346\n",
      "Epoch 41/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3823.2837 - val_loss: 3880.0020\n",
      "Epoch 42/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3822.3623 - val_loss: 3882.9355\n",
      "Epoch 43/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3822.2827 - val_loss: 3879.3147\n",
      "Epoch 44/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3823.2939 - val_loss: 3876.2339\n",
      "Epoch 45/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3825.9673 - val_loss: 3877.1257\n",
      "Epoch 46/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3829.6953 - val_loss: 3879.5261\n",
      "Epoch 47/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3823.9780 - val_loss: 3874.7612\n",
      "Epoch 48/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3825.3237 - val_loss: 3881.0652\n",
      "Epoch 49/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3824.7507 - val_loss: 3876.4753\n",
      "Epoch 50/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3828.6846 - val_loss: 3875.5903\n",
      "Epoch 51/100\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 3824.4392 - val_loss: 3876.9976\n",
      "Epoch 52/100\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 3821.8188 - val_loss: 3874.8638\n",
      "Epoch 53/100\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 3827.9653 - val_loss: 3874.6111\n",
      "Epoch 54/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3824.3965 - val_loss: 3879.0129\n",
      "Epoch 55/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3824.1978 - val_loss: 3877.2766\n",
      "Epoch 56/100\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 3823.9368 - val_loss: 3883.6326\n",
      "Epoch 57/100\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 3824.2002 - val_loss: 3877.3000\n",
      "Epoch 58/100\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 3822.2681 - val_loss: 3875.3916\n",
      "Epoch 59/100\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 3823.9521 - val_loss: 3879.4448\n",
      "Epoch 60/100\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 3825.2163 - val_loss: 3879.6936\n",
      "Epoch 61/100\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 3821.8625 - val_loss: 3879.9534\n",
      "Epoch 62/100\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 3823.6338 - val_loss: 3878.1509\n",
      "Epoch 63/100\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 3824.4814 - val_loss: 3877.7290\n",
      "Epoch 64/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3825.8540 - val_loss: 3874.9893\n",
      "Epoch 65/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3824.5178 - val_loss: 3876.8701\n",
      "Epoch 66/100\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 3823.6621 - val_loss: 3878.4290\n",
      "Epoch 67/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3824.0430 - val_loss: 3876.9048\n",
      "Epoch 68/100\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 3824.4846 - val_loss: 3878.0212\n",
      "Epoch 69/100\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 3824.1396 - val_loss: 3876.9800\n",
      "Epoch 70/100\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 3822.8662 - val_loss: 3877.3293\n",
      "Epoch 71/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3821.9319 - val_loss: 3874.0771\n",
      "Epoch 72/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3824.7546 - val_loss: 3874.4788\n",
      "Epoch 73/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3820.0054 - val_loss: 3881.2212\n",
      "Epoch 74/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3825.1204 - val_loss: 3880.2146\n",
      "Epoch 75/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3820.8831 - val_loss: 3876.3657\n",
      "Epoch 76/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3828.2144 - val_loss: 3877.5146\n",
      "Epoch 77/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3822.5303 - val_loss: 3874.9119\n",
      "Epoch 78/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3825.8699 - val_loss: 3876.4297\n",
      "Epoch 79/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3822.4351 - val_loss: 3875.2275\n",
      "Epoch 80/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3825.8652 - val_loss: 3879.2537\n",
      "Epoch 81/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3826.5034 - val_loss: 3876.8718\n",
      "Epoch 82/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3823.8477 - val_loss: 3877.8154\n",
      "Epoch 83/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3823.0061 - val_loss: 3879.4097\n",
      "Epoch 84/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3825.4758 - val_loss: 3875.0356\n",
      "Epoch 85/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3824.2458 - val_loss: 3877.9199\n",
      "Epoch 86/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3824.5864 - val_loss: 3876.7629\n",
      "Epoch 87/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3825.5430 - val_loss: 3876.3215\n",
      "Epoch 88/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3825.4604 - val_loss: 3876.4292\n",
      "Epoch 89/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3823.1326 - val_loss: 3876.3484\n",
      "Epoch 90/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3824.4780 - val_loss: 3878.7256\n",
      "Epoch 91/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3826.4282 - val_loss: 3877.9944\n",
      "Epoch 92/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3825.5137 - val_loss: 3877.7249\n",
      "Epoch 93/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3825.0698 - val_loss: 3879.0710\n",
      "Epoch 94/100\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 3824.1030 - val_loss: 3875.9712\n",
      "Epoch 95/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3822.5681 - val_loss: 3877.0227\n",
      "Epoch 96/100\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 3823.4951 - val_loss: 3876.0039\n",
      "Epoch 97/100\n",
      "211/211 [==============================] - 1s 4ms/step - loss: 3822.6040 - val_loss: 3878.8079\n",
      "Epoch 98/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3823.7585 - val_loss: 3877.4182\n",
      "Epoch 99/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3819.6116 - val_loss: 3877.7324\n",
      "Epoch 100/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3824.8472 - val_loss: 3877.2861\n",
      "59/59 [==============================] - 2s 2ms/step\n",
      "Mean Squared Error: 4084.3665521731436\n",
      "Root Mean Squared Error: 63.909049063283234\n"
     ]
    }
   ],
   "source": [
    "# Load your data\n",
    "df = pd.read_csv(\"jobs_in_data.csv\")\n",
    "\n",
    "# Split the 'salary_in_usd' column by 1000\n",
    "df['salary_in_usd'] /= 1000\n",
    "\n",
    "# Define features and target variable\n",
    "X_numerical = df.select_dtypes(include=np.number).drop(columns=[\"salary_in_usd\", \"salary\"])  # Select only numeric columns\n",
    "y = df[\"salary_in_usd\"]\n",
    "\n",
    "# Scale numerical data\n",
    "scaler = StandardScaler()\n",
    "X_numerical_scaled = scaler.fit_transform(X_numerical)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train_num, X_test_num, y_train, y_test = train_test_split(X_numerical_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the input data to include timestep dimension\n",
    "X_train_reshaped = X_train_num.reshape(X_train_num.shape[0], X_train_num.shape[1], 1)\n",
    "X_test_reshaped = X_test_num.reshape(X_test_num.shape[0], X_test_num.shape[1], 1)\n",
    "\n",
    "# Define LSTM model\n",
    "input_layer = Input(shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]))\n",
    "lstm_layer = LSTM(128, return_sequences=True)(input_layer)\n",
    "dropout_layer = Dropout(0.2)(lstm_layer)\n",
    "lstm_layer2 = LSTM(64)(dropout_layer)\n",
    "output_layer = Dense(1, activation='linear')(lstm_layer2)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", np.sqrt(mse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mean Squared Error: 4084.3665521731436\n",
    "Root Mean Squared Error: 63.909049063283234"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b545449918267e7a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adding a column of the salary ratio:\n",
    "['salary_in_usd'] / ['salary'] \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f6bffea0c4c40d5"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "211/211 [==============================] - 4s 8ms/step - loss: 22370.7793 - val_loss: 19209.8320\n",
      "Epoch 2/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 17320.1797 - val_loss: 15788.8789\n",
      "Epoch 3/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 14359.8555 - val_loss: 13198.5811\n",
      "Epoch 4/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 12040.6309 - val_loss: 11127.8867\n",
      "Epoch 5/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 10166.0557 - val_loss: 9444.0596\n",
      "Epoch 6/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 8651.4736 - val_loss: 8090.2856\n",
      "Epoch 7/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 7435.4849 - val_loss: 7009.4585\n",
      "Epoch 8/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 6476.0732 - val_loss: 6162.5493\n",
      "Epoch 9/100\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 5733.1802 - val_loss: 5516.9287\n",
      "Epoch 10/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 5171.1353 - val_loss: 5035.5933\n",
      "Epoch 11/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 4756.9219 - val_loss: 4687.0542\n",
      "Epoch 12/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 4461.8740 - val_loss: 4441.8369\n",
      "Epoch 13/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 4260.4175 - val_loss: 4278.9116\n",
      "Epoch 14/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 4126.3423 - val_loss: 4173.6968\n",
      "Epoch 15/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 4042.7229 - val_loss: 4109.2422\n",
      "Epoch 16/100\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 3993.3284 - val_loss: 4072.6970\n",
      "Epoch 17/100\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 3965.9475 - val_loss: 4053.1602\n",
      "Epoch 18/100\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 3951.6895 - val_loss: 4042.2288\n",
      "Epoch 19/100\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 3904.7939 - val_loss: 3907.4153\n",
      "Epoch 20/100\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 3834.2466 - val_loss: 3883.6758\n",
      "Epoch 21/100\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 3816.0991 - val_loss: 3872.2034\n",
      "Epoch 22/100\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 3804.0376 - val_loss: 3855.5002\n",
      "Epoch 23/100\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 3791.4944 - val_loss: 3843.5466\n",
      "Epoch 24/100\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 3784.9209 - val_loss: 3841.4243\n",
      "Epoch 25/100\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 3778.2908 - val_loss: 3838.6338\n",
      "Epoch 26/100\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 3779.4373 - val_loss: 3836.1592\n",
      "Epoch 27/100\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 3773.9673 - val_loss: 3834.7053\n",
      "Epoch 28/100\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 3777.7598 - val_loss: 3832.5337\n",
      "Epoch 29/100\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 3774.1538 - val_loss: 3835.3958\n",
      "Epoch 30/100\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 3770.3872 - val_loss: 3827.2412\n",
      "Epoch 31/100\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 3765.5994 - val_loss: 3823.5039\n",
      "Epoch 32/100\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 3759.2722 - val_loss: 3820.8889\n",
      "Epoch 33/100\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 3752.8059 - val_loss: 3812.7488\n",
      "Epoch 34/100\n",
      "211/211 [==============================] - 2s 7ms/step - loss: 3741.2117 - val_loss: 3809.5952\n",
      "Epoch 35/100\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 3741.5273 - val_loss: 3805.7427\n",
      "Epoch 36/100\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 3735.2534 - val_loss: 3799.1328\n",
      "Epoch 37/100\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 3731.0144 - val_loss: 3795.0837\n",
      "Epoch 38/100\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 3725.8691 - val_loss: 3791.4717\n",
      "Epoch 39/100\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 3718.6196 - val_loss: 3787.3430\n",
      "Epoch 40/100\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 3712.0120 - val_loss: 3783.9375\n",
      "Epoch 41/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3707.3657 - val_loss: 3784.2891\n",
      "Epoch 42/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3707.4121 - val_loss: 3782.6082\n",
      "Epoch 43/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3700.5642 - val_loss: 3780.6099\n",
      "Epoch 44/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3699.2632 - val_loss: 3781.1638\n",
      "Epoch 45/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3695.4812 - val_loss: 3781.8115\n",
      "Epoch 46/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3698.4448 - val_loss: 3781.4963\n",
      "Epoch 47/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3701.3633 - val_loss: 3782.1902\n",
      "Epoch 48/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3694.2771 - val_loss: 3783.8994\n",
      "Epoch 49/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3694.5728 - val_loss: 3782.0303\n",
      "Epoch 50/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3693.9539 - val_loss: 3782.3381\n",
      "Epoch 51/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3693.2891 - val_loss: 3781.8691\n",
      "Epoch 52/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3692.1592 - val_loss: 3779.7402\n",
      "Epoch 53/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3691.6443 - val_loss: 3782.0647\n",
      "Epoch 54/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3691.9934 - val_loss: 3782.3494\n",
      "Epoch 55/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3688.0979 - val_loss: 3781.4099\n",
      "Epoch 56/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3688.8389 - val_loss: 3781.0430\n",
      "Epoch 57/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3695.9058 - val_loss: 3781.2129\n",
      "Epoch 58/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3688.4695 - val_loss: 3778.8110\n",
      "Epoch 59/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3687.7942 - val_loss: 3774.6826\n",
      "Epoch 60/100\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 3688.1711 - val_loss: 3774.4263\n",
      "Epoch 61/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3685.2620 - val_loss: 3776.5583\n",
      "Epoch 62/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3688.2209 - val_loss: 3777.1101\n",
      "Epoch 63/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3686.8940 - val_loss: 3775.9465\n",
      "Epoch 64/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3685.3215 - val_loss: 3775.4846\n",
      "Epoch 65/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3683.8696 - val_loss: 3774.5620\n",
      "Epoch 66/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3683.1960 - val_loss: 3774.6980\n",
      "Epoch 67/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3680.6702 - val_loss: 3772.6245\n",
      "Epoch 68/100\n",
      "211/211 [==============================] - 1s 7ms/step - loss: 3681.8865 - val_loss: 3773.8428\n",
      "Epoch 69/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3680.5588 - val_loss: 3774.5867\n",
      "Epoch 70/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3681.7871 - val_loss: 3774.1807\n",
      "Epoch 71/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3682.8774 - val_loss: 3774.7429\n",
      "Epoch 72/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3679.7390 - val_loss: 3774.4053\n",
      "Epoch 73/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3683.0146 - val_loss: 3776.0935\n",
      "Epoch 74/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3678.0393 - val_loss: 3777.8525\n",
      "Epoch 75/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3679.3123 - val_loss: 3779.0847\n",
      "Epoch 76/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3678.8865 - val_loss: 3777.5320\n",
      "Epoch 77/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3677.6868 - val_loss: 3780.0815\n",
      "Epoch 78/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3678.4568 - val_loss: 3779.6335\n",
      "Epoch 79/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3677.9534 - val_loss: 3779.6292\n",
      "Epoch 80/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3677.8735 - val_loss: 3779.8071\n",
      "Epoch 81/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3677.4546 - val_loss: 3776.7625\n",
      "Epoch 82/100\n",
      "211/211 [==============================] - 1s 5ms/step - loss: 3678.4695 - val_loss: 3777.7134\n",
      "Epoch 83/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3679.8220 - val_loss: 3774.5554\n",
      "Epoch 84/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3678.4832 - val_loss: 3775.7888\n",
      "Epoch 85/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3678.2886 - val_loss: 3776.2312\n",
      "Epoch 86/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3677.7869 - val_loss: 3776.1038\n",
      "Epoch 87/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3677.3606 - val_loss: 3777.1294\n",
      "Epoch 88/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3675.9612 - val_loss: 3771.0725\n",
      "Epoch 89/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3665.1172 - val_loss: 3765.1384\n",
      "Epoch 90/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3660.8826 - val_loss: 3747.9272\n",
      "Epoch 91/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3648.6963 - val_loss: 3744.8794\n",
      "Epoch 92/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3639.9048 - val_loss: 3733.5127\n",
      "Epoch 93/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3619.8921 - val_loss: 3695.5684\n",
      "Epoch 94/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3532.9407 - val_loss: 3585.4226\n",
      "Epoch 95/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3457.7896 - val_loss: 3547.3364\n",
      "Epoch 96/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3434.7153 - val_loss: 3529.5891\n",
      "Epoch 97/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3429.6196 - val_loss: 3514.1079\n",
      "Epoch 98/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3419.8806 - val_loss: 3503.9954\n",
      "Epoch 99/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3409.7051 - val_loss: 3501.0188\n",
      "Epoch 100/100\n",
      "211/211 [==============================] - 1s 6ms/step - loss: 3409.9873 - val_loss: 3498.5129\n",
      "59/59 [==============================] - 2s 2ms/step\n",
      "Mean Squared Error: 3694.67274635093\n",
      "Root Mean Squared Error: 60.78381977426994\n"
     ]
    }
   ],
   "source": [
    "# Load your data\n",
    "df = pd.read_csv(\"jobs_in_data.csv\")\n",
    "\n",
    "# Split the 'salary_in_usd' column by 1000\n",
    "df['salary_in_usd'] /= 1000\n",
    "\n",
    "# Calculate the ratio of \"salary_in_usd\" to \"salary\"\n",
    "df['salary_ratio'] = df['salary_in_usd'] / df['salary']\n",
    "\n",
    "# Define features and target variable\n",
    "X_numerical = df.select_dtypes(include=np.number).drop(columns=[\"salary_in_usd\", \"salary\"])  # Select only numeric columns\n",
    "y = df[\"salary_in_usd\"]\n",
    "\n",
    "# Scale numerical data\n",
    "scaler = StandardScaler()\n",
    "X_numerical_scaled = scaler.fit_transform(X_numerical)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train_num, X_test_num, y_train, y_test = train_test_split(X_numerical_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the input data to include timestep dimension\n",
    "X_train_reshaped = X_train_num.reshape(X_train_num.shape[0], X_train_num.shape[1], 1)\n",
    "X_test_reshaped = X_test_num.reshape(X_test_num.shape[0], X_test_num.shape[1], 1)\n",
    "\n",
    "# Define LSTM model\n",
    "input_layer = Input(shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]))\n",
    "lstm_layer = LSTM(128, return_sequences=True)(input_layer)\n",
    "dropout_layer = Dropout(0.2)(lstm_layer)\n",
    "lstm_layer2 = LSTM(64)(dropout_layer)\n",
    "output_layer = Dense(1, activation='linear')(lstm_layer2)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", np.sqrt(mse))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T09:59:26.316035400Z",
     "start_time": "2024-02-27T09:57:13.396474500Z"
    }
   },
   "id": "c168b4c71b26642c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mean Squared Error: 3694.67274635093\n",
    "Root Mean Squared Error: 60.78381977426994"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2cb27b357a0a0af"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3e3a98dfe63a915a"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 64)           640000    \n",
      "                                                                 \n",
      " spatial_dropout1d (Spatial  (None, 100, 64)           0         \n",
      " Dropout1D)                                                      \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 100)               66000     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 706101 (2.69 MB)\n",
      "Trainable params: 706101 (2.69 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 67356, 7484\n  y sizes: 7484\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 60\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28mprint\u001B[39m(model\u001B[38;5;241m.\u001B[39msummary())\n\u001B[0;32m     59\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m---> 60\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mX_train_text_seq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train_numerical_scaled\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mX_test_text_seq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_test_numerical_scaled\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mEarlyStopping\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpatience\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n\u001B[0;32m     63\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict([X_test_text_seq, X_test_numerical_scaled])\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1960\u001B[0m, in \u001B[0;36m_check_data_cardinality\u001B[1;34m(data)\u001B[0m\n\u001B[0;32m   1953\u001B[0m     msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m  \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m sizes: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m   1954\u001B[0m         label,\n\u001B[0;32m   1955\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\n\u001B[0;32m   1956\u001B[0m             \u001B[38;5;28mstr\u001B[39m(i\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mnest\u001B[38;5;241m.\u001B[39mflatten(single_data)\n\u001B[0;32m   1957\u001B[0m         ),\n\u001B[0;32m   1958\u001B[0m     )\n\u001B[0;32m   1959\u001B[0m msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMake sure all arrays contain the same number of samples.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1960\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n",
      "\u001B[1;31mValueError\u001B[0m: Data cardinality is ambiguous:\n  x sizes: 67356, 7484\n  y sizes: 7484\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(\"jobs_in_data.csv\")\n",
    "\n",
    "# Drop the \"salary\" column\n",
    "# df = df.drop(columns=[\"salary\"])\n",
    "\n",
    "# Split data into features and target variable\n",
    "X = df.drop(columns=[\"salary_in_usd\",\"salary\"])  # Features\n",
    "y = df[\"salary_in_usd\"]  # Target variable\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess numerical data\n",
    "numerical_columns = X.select_dtypes(include=np.number).columns\n",
    "scaler = StandardScaler()\n",
    "X_train_numerical_scaled = scaler.fit_transform(X_train[numerical_columns])\n",
    "X_test_numerical_scaled = scaler.transform(X_test[numerical_columns])\n",
    "\n",
    "# Preprocess text data\n",
    "max_words = 10000  # Maximum number of words to tokenize\n",
    "max_sequence_length = 100  # Maximum sequence length for padding\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train.select_dtypes(include=[object]).values.flatten())\n",
    "\n",
    "X_train_text_seq = tokenizer.texts_to_sequences(X_train.select_dtypes(include=[object]).values.flatten())\n",
    "X_train_text_seq = pad_sequences(X_train_text_seq, maxlen=max_sequence_length)\n",
    "\n",
    "X_test_text_seq = tokenizer.texts_to_sequences(X_test.select_dtypes(include=[object]).values.flatten())\n",
    "X_test_text_seq = pad_sequences(X_test_text_seq, maxlen=max_sequence_length)\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=64, input_length=max_sequence_length),\n",
    "    SpatialDropout1D(0.2),\n",
    "    LSTM(100),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Print model summary\n",
    "print(\"Model summary:\")\n",
    "print(model.summary())\n",
    "\n",
    "# Train the model\n",
    "history = model.fit([X_train_text_seq, X_train_numerical_scaled], y_train, epochs=20, batch_size=64, validation_data=([X_test_text_seq, X_test_numerical_scaled], y_test), callbacks=[EarlyStopping(patience=3)])\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict([X_test_text_seq, X_test_numerical_scaled])\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", np.sqrt(mse))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T09:37:33.736738400Z",
     "start_time": "2024-02-27T09:37:31.116862700Z"
    }
   },
   "id": "40536bf48621ebc7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
